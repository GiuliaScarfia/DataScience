{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_CzSoFy1aps"
   },
   "source": [
    "# Introduction to Computer Vision with Convolutional Neural Networks\n",
    "\n",
    "This notebook provides a basic introduction to computer vision tasks using deep learning, specifically focusing on Convolutional Neural Networks (CNNs). We'll explore:\n",
    "\n",
    "1. What is computer vision?\n",
    "2. How CNNs work\n",
    "3. Building two MNIST digit classifiers with PyTorch Lightning:\n",
    "   - A simple Multi-Layer Perceptron (MLP) model\n",
    "   - A CNN-based model\n",
    "4. Comparing results\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tQq1Lss1apx"
   },
   "source": [
    "## 1. What is Computer Vision?\n",
    "\n",
    "Computer vision is a field of artificial intelligence that enables computers to derive meaningful information from digital images, videos, and other visual inputs. It aims to automate tasks that the human visual system can do.\n",
    "\n",
    "### Key Applications of Computer Vision:\n",
    "\n",
    "- **Image Classification**: Categorizing images into predefined classes\n",
    "- **Object Detection**: Identifying and localizing objects within images\n",
    "- **Image Segmentation**: Dividing images into meaningful segments or regions\n",
    "- **Facial Recognition**: Identifying or verifying individuals based on facial features\n",
    "- **Optical Character Recognition (OCR)**: Converting typed, handwritten, or printed text into machine-encoded text\n",
    "- **Autonomous Vehicles**: Helping cars \"see\" and navigate their environment\n",
    "- **Medical Imaging Analysis**: Assisting in diagnosis and treatment planning\n",
    "\n",
    "### Evolution of Computer Vision:\n",
    "\n",
    "1. **Traditional Methods**: Used hand-crafted features (e.g., edge detection, HOG, SIFT) and traditional machine learning algorithms\n",
    "2. **Deep Learning Revolution**: Introduced end-to-end learning with neural networks trained on large datasets\n",
    "3. **Convolutional Neural Networks**: Became the dominant approach for most computer vision tasks\n",
    "\n",
    "Today's notebook focuses on CNNs, which have revolutionized the field of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJGKdWXw1apz"
   },
   "source": [
    "## 2. Understanding Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized neural networks designed to process data with grid-like topology, such as images. They are inspired by the organization of the animal visual cortex, where individual neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.\n",
    "\n",
    "### Key Components of CNNs:\n",
    "\n",
    "#### 2.1 Convolutional Layers\n",
    "\n",
    "Convolutional layers are the core building blocks of CNNs. The layer's parameters consist of a set of learnable **filters** (or kernels).\n",
    "\n",
    "**How Convolution Works**:\n",
    "- Each filter is small spatially (along width and height) but extends through the full depth of the input\n",
    "- During the forward pass, each filter slides (or convolves) across the width and height of the input\n",
    "- At each position, a dot product is computed between the filter parameters and the input\n",
    "- This produces a 2D activation map showing the filter's responses at every spatial position\n",
    "\n",
    "Key advantages:\n",
    "- **Parameter Sharing**: Each filter is reused across the entire image\n",
    "- **Local Connectivity**: Each neuron connects only to a small region of the input\n",
    "\n",
    "Here is a 1 dimensional example (i.e. a black and white image input)\n",
    "\n",
    "![convolution.png](https://maucher.home.hdm-stuttgart.de/Pics/gif/same_padding_no_strides.gif)\n",
    "\n",
    "Here is the multi dimensional generalization of a convolutional layer (i.e. a multichannel image, like a RGB image, or a multispectral image, i.e. a multispectral xray image)\n",
    "\n",
    "![convolution2.png](https://animatedai.github.io/media/convolution-animation-3x3-kernel-same-padding.gif)\n",
    "\n",
    "#### 2.2 Pooling Layers\n",
    "\n",
    "Pooling layers reduce the spatial dimensions (width and height) of the input volume. This serves two main purposes:\n",
    "- Reducing the number of parameters and computation in the network\n",
    "- Controlling overfitting\n",
    "\n",
    "**Common Pooling Operations**:\n",
    "- **Max Pooling**: Takes the maximum value from a window of the feature map\n",
    "- **Average Pooling**: Takes the average value from a window of the feature map\n",
    "\n",
    "![pooling.png](https://www.google.com/url?sa=i&url=https%3A%2F%2Fpub.towardsai.net%2Fintroduction-to-pooling-layers-in-cnn-dafe61eabe34&psig=AOvVaw0lJjpwcpsQ2b-X5qP6-Koi&ust=1742981314135000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCNjo4sP1pIwDFQAAAAAdAAAAABAE)\n",
    "\n",
    "#### 2.3 Fully Connected Layers\n",
    "\n",
    "After several convolutional and pooling layers, the high-level reasoning in the neural network is done via fully connected layers:\n",
    "- Neurons in these layers have connections to all activations in the previous layer\n",
    "- Used at the end of the network to perform classification based on features extracted by previous layers\n",
    "\n",
    "#### 2.4 Activation Functions\n",
    "\n",
    "Non-linear activation functions introduce non-linearity into the network, allowing it to learn complex patterns:\n",
    "- **ReLU (Rectified Linear Unit)**: Most commonly used, f(x) = max(0, x)\n",
    "- **Sigmoid**: Maps values to range [0,1], used for binary classification\n",
    "- **Softmax**: Used for multi-class classification in the output layer\n",
    "\n",
    "### Architectural Pattern of a Typical CNN:\n",
    "\n",
    "```\n",
    "Input Image → [Conv → ReLU → Pool] × N → [FC → ReLU] × M → FC → Softmax\n",
    "```\n",
    "\n",
    "Where:\n",
    "- N is the number of convolutional blocks (typically 2-5)\n",
    "- M is the number of fully connected layers (typically 0-2)\n",
    "\n",
    "![cnn_architecture](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/10/90650dnn2.webp)\n",
    "\n",
    "\n",
    "### Why CNNs Work Well for Images:\n",
    "\n",
    "1. **Hierarchical Feature Learning**: Lower layers detect simple features (edges, corners), while deeper layers detect complex features (shapes, objects)\n",
    "2. **Translation Invariance**: Can recognize patterns regardless of their position in the image\n",
    "3. **Efficient Parameter Usage**: Parameter sharing drastically reduces the number of parameters compared to fully connected networks\n",
    "4. **Built-in Feature Engineering**: Automatically learns relevant features from raw pixel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moaM8NCM1ap0"
   },
   "source": [
    "## 3. Setting Up Our Environment\n",
    "\n",
    "Let's install and import the necessary libraries for our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ-t1Lq61ap1"
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "! pip install torch torchvision lightning matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CPVEBfp1ap3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHSiZhCO1ap4"
   },
   "source": [
    "## 4. Preparing the MNIST Dataset\n",
    "\n",
    "The MNIST dataset consists of 28×28 grayscale images of handwritten digits (0-9). It's a standard benchmark dataset in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR6JYLPe1ap5"
   },
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # MNIST mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Create a PyTorch Lightning DataModule for MNIST\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"./data\", batch_size=64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download data if needed (only once)\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val/test datasets\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = datasets.MNIST(\n",
    "                self.data_dir, train=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(\n",
    "                self.data_dir, train=False, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbNU2Wi31ap6"
   },
   "source": [
    "Let's visualize some samples from the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BI7MrdU1ap7"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Display 10 random images from the dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = np.random.randint(len(mnist_train))\n",
    "    img, label = mnist_train[idx]\n",
    "    img = img.squeeze().numpy()  # Remove channel dimension and convert to numpy\n",
    "\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Digit: {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5r0EPns1ap7"
   },
   "source": [
    "## 5. Building Model 1: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "First, let's create a simple MLP model that flattens the input image and passes it through fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejHvVAmS1ap8"
   },
   "outputs": [],
   "source": [
    "class MNISTClassifierMLP(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Define the network architecture\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)  # 10 output classes (digits 0-9)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, 28, 28]\n",
    "        x = self.flatten(x)  # Shape: [batch_size, 784]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # Shape: [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log training metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log test metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': acc, 'preds': preds, 'targets': y}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7K-RwX71ap8"
   },
   "source": [
    "## 6. Building Model 2: Convolutional Neural Network (CNN)\n",
    "\n",
    "Now, let's create a CNN model that uses convolutional layers to extract features from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3TMgm_L1ap9"
   },
   "outputs": [],
   "source": [
    "class MNISTClassifierCNN(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=32, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Convolutional layers\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the size of features after convolutions and pooling\n",
    "        # After 2 pooling layers with stride 2, spatial dimensions are reduced by factor of 4\n",
    "        # So 28x28 becomes 7x7, and we have 32 feature maps\n",
    "        self.fc_input_size = 32 * 7 * 7\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)  # 10 output classes (digits 0-9)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, 28, 28]\n",
    "\n",
    "        # First convolutional block\n",
    "        x = F.relu(self.conv1(x))  # Shape: [batch_size, 32, 28, 28]\n",
    "        x = self.pool(x)  # Shape: [batch_size, 32, 14, 14]\n",
    "\n",
    "        # Second convolutional block\n",
    "        x = F.relu(self.conv2(x))  # Shape: [batch_size, 64, 14, 14]\n",
    "        x = self.pool(x)  # Shape: [batch_size, 64, 7, 7]\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(-1, self.fc_input_size)  # Shape: [batch_size, 64*7*7]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Shape: [batch_size, 10]\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Reusing the same training, validation, and test steps as the MLP model\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return {\"test_loss\": loss, \"test_acc\": acc, \"preds\": preds, \"targets\": y}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW27dwel1ap9"
   },
   "source": [
    "## 7. Training and Evaluating Models\n",
    "\n",
    "Now let's train and evaluate both models to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juyAOSzX1ap9"
   },
   "outputs": [],
   "source": [
    "# Setup the data module\n",
    "data_module = MNISTDataModule(batch_size=128)\n",
    "\n",
    "# Define a function to train and evaluate a model\n",
    "def train_and_evaluate(model, model_name, max_epochs=5):\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        accelerator='auto',  # Use GPU if available\n",
    "        logger=True,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # Test the model\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    test_results = trainer.test(model, data_module)\n",
    "\n",
    "    return model, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3n2q82a1ap9"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the MLP model\n",
    "mlp_model = MNISTClassifierMLP(hidden_size=64, learning_rate=0.001)\n",
    "mlp_model, mlp_results = train_and_evaluate(mlp_model, \"MLP Model\", max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmW4UW_41ap-"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the CNN model\n",
    "cnn_model = MNISTClassifierCNN(hidden_size=32, learning_rate=0.001)\n",
    "cnn_model, cnn_results = train_and_evaluate(cnn_model, \"CNN Model\", max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PliXtzaj1ap-"
   },
   "source": [
    "## 8. Compare Results\n",
    "\n",
    "Let's compare the performance of both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyACsUpW1ap-"
   },
   "outputs": [],
   "source": [
    "# Print result comparison\n",
    "print(\"-\" * 50)\n",
    "print(\"Model Comparison on MNIST Test Set:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"MLP Model - Accuracy: {mlp_results[0]['test_acc']:.4f}, Loss: {mlp_results[0]['test_loss']:.4f}\")\n",
    "print(f\"CNN Model - Accuracy: {cnn_results[0]['test_acc']:.4f}, Loss: {cnn_results[0]['test_loss']:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate improvement\n",
    "acc_improvement = (cnn_results[0]['test_acc'] - mlp_results[0]['test_acc']) * 100\n",
    "print(f\"CNN accuracy improvement: {acc_improvement:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-yizCqK1ap-"
   },
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "# Print summary of MLP model\n",
    "print(\"MLP Model Summary:\")\n",
    "summary(mlp_model, input_size=(1, 28, 28))\n",
    "\n",
    "# Print summary of CNN model\n",
    "print(\"\\nCNN Model Summary:\")\n",
    "summary(cnn_model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVOMpVf41ap-"
   },
   "source": [
    "## 9. Visualize Predictions\n",
    "\n",
    "Let's visualize some predictions from the CNN model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oao1dep1ap-"
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, num_samples=10):\n",
    "    # Get a batch of test data\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    batch = next(iter(test_loader))\n",
    "    images, labels = batch\n",
    "\n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(images)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # Plot images with predictions\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = images[i].squeeze().cpu().numpy()\n",
    "        label = labels[i].item()\n",
    "        pred = preds[i].item()\n",
    "\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        title_color = 'green' if pred == label else 'red'\n",
    "        ax.set_title(f'True: {label}, Pred: {pred}', color=title_color)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions from CNN model\n",
    "print(\"CNN Model Predictions:\")\n",
    "show_predictions(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-jcmbsp1ap_"
   },
   "source": [
    "## 10. Visualizing Learned Features\n",
    "\n",
    "Let's visualize some of the filters (kernels) learned by the first convolutional layer of our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vn2GVLP1ap_"
   },
   "outputs": [],
   "source": [
    "# Get the weights of the first convolutional layer\n",
    "conv1_weights = cnn_model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "# Plot the first 16 filters\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    # Each filter is 3D: [channels, height, width], but MNIST has only 1 channel\n",
    "    filter_img = conv1_weights[i, 0]\n",
    "    axes[i].imshow(filter_img, cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('First 16 Filters of First Convolutional Layer', y=0.95, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swLfDfce1ap_"
   },
   "outputs": [],
   "source": [
    "def visualize_cnn_layers(model, image):\n",
    "    \"\"\"\n",
    "    Visualize the activations of all layers of a CNN model for a single input image.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    hooks = []\n",
    "\n",
    "    # Register hooks to capture activations\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, (nn.Conv2d, nn.MaxPool2d, nn.Linear)):\n",
    "            hooks.append(layer.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    # Plot activations\n",
    "    for i, activation in enumerate(activations):\n",
    "        if len(activation.shape) == 4:  # Convolutional or pooling layer\n",
    "            num_filters = activation.shape[1]\n",
    "            fig, axes = plt.subplots(1, num_filters, figsize=(15, 5))\n",
    "            fig.suptitle(f\"Layer {i + 1} Activations\", fontsize=16)\n",
    "            for j in range(num_filters):\n",
    "                ax = axes[j]\n",
    "                ax.imshow(activation[0, j].cpu().numpy(), cmap=\"viridis\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Last Layer Activations\")\n",
    "    a = activations[-1][0].cpu().numpy()\n",
    "    a = torch.softmax(torch.tensor(a), dim=0).numpy()\n",
    "    plt.bar(x=range(len(a)), height=a)\n",
    "    plt.show()\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "\n",
    "# Select one example from the test dataset\n",
    "test_loader = data_module.test_dataloader()\n",
    "example_image, _ = next(iter(test_loader))\n",
    "\n",
    "# Visualize activations\n",
    "visualize_cnn_layers(cnn_model, example_image[0])  # Take the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk-cnziW1aqA"
   },
   "outputs": [],
   "source": [
    "visualize_cnn_layers(cnn_model, example_image[100])  # Take the first image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYn8tFAx1aqA"
   },
   "source": [
    "## 11. Analysis and Conclusions\n",
    "\n",
    "Based on our experiments, we can draw several conclusions about CNNs for computer vision tasks:\n",
    "\n",
    "1. **Performance Comparison**: The CNN model typically achieves higher accuracy than the MLP model on the MNIST dataset. This demonstrates the effectiveness of convolutional layers for image processing tasks.\n",
    "\n",
    "2. **Why CNNs Outperform MLPs for Images**:\n",
    "   - **Spatial Hierarchy**: CNNs preserve and leverage the spatial structure of images\n",
    "   - **Feature Extraction**: Convolutional layers automatically learn relevant features\n",
    "   - **Parameter Efficiency**: CNNs use significantly fewer parameters than MLPs for the same task\n",
    "   - **Translation Invariance**: CNNs can recognize patterns regardless of their position\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - Our MLP model flattens the 28×28 image into a 784-dimensional vector, losing spatial information\n",
    "   - Our CNN model preserves the 2D structure and gradually extracts hierarchical features\n",
    "\n",
    "4. **Training Speed**: Although CNNs are more complex architecturally, they often converge faster and to better solutions for image tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaOULc401aqA"
   },
   "source": [
    "## 12. Exercise: Fashion MNIST Classification\n",
    "\n",
    "**Your Task**: Adapt the CNN model we've built to classify the Fashion MNIST dataset.\n",
    "\n",
    "The Fashion MNIST dataset consists of 28×28 grayscale images of 10 fashion categories:\n",
    "- 0: T-shirt/top\n",
    "- 1: Trouser\n",
    "- 2: Pullover\n",
    "- 3: Dress\n",
    "- 4: Coat\n",
    "- 5: Sandal\n",
    "- 6: Shirt\n",
    "- 7: Sneaker\n",
    "- 8: Bag\n",
    "- 9: Ankle boot\n",
    "\n",
    "**Steps**:\n",
    "1. Load the Fashion MNIST dataset instead of regular MNIST\n",
    "2. Adapt the CNN model if necessary (you might want to use more filters or deeper architecture for this more complex dataset)\n",
    "3. Train and evaluate the model\n",
    "4. Visualize the results and compare with the digit classification results\n",
    "\n",
    "**Hint**: You can load Fashion MNIST using `torchvision.datasets.FashionMNIST` instead of `datasets.MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoqeH-lp1aqA"
   },
   "outputs": [],
   "source": [
    "# Exercise starter code: Loading Fashion MNIST dataset\n",
    "fashion_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))  # Fashion MNIST mean and std\n",
    "])\n",
    "\n",
    "# Define the Fashion MNIST class names for better visualization\n",
    "fashion_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# TODO: Create a FashionMNISTDataModule similar to MNISTDataModule\n",
    "\n",
    "# TODO: Implement FashionMNISTClassifierCNN based on MNISTClassifierCNN\n",
    "\n",
    "# TODO: Train and evaluate your model on Fashion MNIST\n",
    "\n",
    "# TODO: Visualize some predictions and analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrGgDJ7_1aqA"
   },
   "source": [
    "## 13. Further Learning Resources\n",
    "\n",
    "To continue your journey in computer vision with CNNs, here are some valuable resources:\n",
    "\n",
    "1. **Books**:\n",
    "   - \"Deep Learning\" by Goodfellow, Bengio, and Courville (especially Chapter 9 on CNNs) (it's FREE! :0) [link](https://www.deeplearningbook.org/)\n",
    "   - \"Computer Vision: Algorithms and Applications\" by Richard Szeliski [link](https://link.springer.com/book/10.1007/978-3-030-34372-9)\n",
    "\n",
    "2. **Online Courses**:\n",
    "   - CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)\n",
    "      [link](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n",
    "   - Deep Learning Specialization by Andrew Ng (Coursera)\n",
    "      [link](https://www.coursera.org/specializations/deep-learning?utm_medium=sem&utm_source=gg&utm_campaign=b2c_emea_deep-learning_deeplearning-ai_ftcof_specializations_cx_dr_bau_gg_pmax_pr_s1_en_m_hyb_24-02_x&campaignid=21028581571&adgroupid=&device=c&keyword=&matchtype=&network=x&devicemodel=&creativeid=&assetgroupid=6490696594&targetid=&extensionid=&placement=&gad_source=1&gclid=Cj0KCQjw-e6-BhDmARIsAOxxlxW7_bhWiZpU1Xx0yerMje5jqyr3PBhwzhTqERo8CAYRx4cpnbq27uAaAtOSEALw_wcB)\n",
    "\n",
    "3. **Papers**:\n",
    "   - AlexNet: \"ImageNet Classification with Deep Convolutional Neural Networks\" (Krizhevsky et al., 2012) [link](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "   - VGGNet: \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" (Simonyan & Zisserman, 2014) [link](https://arxiv.org/abs/1409.1556)\n",
    "   - ResNet: \"Deep Residual Learning for Image Recognition\" (He et al., 2015) [link](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "\n",
    "4. **PyTorch Resources**:\n",
    "   - PyTorch Tutorials: [link](https://pytorch.org/tutorials/)\n",
    "   - PyTorch Lightning Docs: [link](https://lightning.ai/docs/pytorch/stable/)\n",
    "\n",
    "5. **Datasets to Explore**:\n",
    "   - CIFAR-10 and CIFAR-100: Natural images with 10 and 100 classes\n",
    "   - ImageNet: Large-scale dataset with 1000 classes\n",
    "   - COCO: Dataset for object detection, segmentation, and captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypeX3NsP1aqA"
   },
   "source": [
    "## 14. Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Introduction to Computer Vision**: Understanding what computer vision is and its key applications\n",
    "\n",
    "2. **Convolutional Neural Networks**: Detailed explanation of CNNs, their components, and why they work well for images\n",
    "\n",
    "3. **MNIST Classification**: Built two models (MLP and CNN) to classify handwritten digits\n",
    "\n",
    "4. **Model Comparison**: Evaluated and compared both models, showing the advantages of CNNs for image tasks\n",
    "\n",
    "5. **Visualization**: Explored the model's predictions and learned features\n",
    "\n",
    "6. **Exercise**: Provided a template for adapting our CNN model to the Fashion MNIST dataset\n",
    "\n",
    "With this foundational knowledge, you're well-equipped to start exploring more complex computer vision tasks and architectures!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
