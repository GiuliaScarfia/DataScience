{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af960ea-2f7e-40d8-bb91-d26969c0f876",
   "metadata": {},
   "source": [
    "HOMEWORK\n",
    "\n",
    "Reimplement the NeuralNetwork class by using the Perceptron class inside it (e.g. a layer is an array of perceptrons)\n",
    "Inside the class extract weights and other info from the Perceptrons to allow vector-matrix efficient multiplications\n",
    "(BONUS) make everything not using matrix multiplications, but rather for loops iterating over list of Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f04db5d-b882-402e-a3a9-37d90bf1d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output con moltiplicazioni di matrici: [0.61596466 4.31992087]\n",
      "Output senza moltiplicazioni di matrici: [np.float64(0.6159646581446045), np.float64(4.31992086687589)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, activation_function):\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def activate(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation_function(z)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation_function):\n",
    "        self.layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            layer = [Perceptron(layers[i], activation_function) for _ in range(layers[i+1])]\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = np.array([neuron.activate(X) for neuron in layer])\n",
    "        return X\n",
    "    \n",
    "    def forward_no_matrix(self, X):  # BONUS: senza moltiplicazioni di matrici\n",
    "        for layer in self.layers:\n",
    "            new_X = []\n",
    "            for neuron in layer:\n",
    "                z = sum(x * w for x, w in zip(X, neuron.weights)) + neuron.bias\n",
    "                new_X.append(neuron.activation_function(z))\n",
    "            X = new_X\n",
    "        return X\n",
    "\n",
    "# Funzione di attivazione di esempio\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Esempio di utilizzo\n",
    "nn = NeuralNetwork([3, 4, 2], relu)\n",
    "X = np.array([0.5, 0.2, 0.8])\n",
    "output = nn.forward(X)\n",
    "output_no_matrix = nn.forward_no_matrix(X)\n",
    "\n",
    "print(\"Output con moltiplicazioni di matrici:\", output)\n",
    "print(\"Output senza moltiplicazioni di matrici:\", output_no_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0ed58-3dc5-480c-bb9b-483121be70e7",
   "metadata": {},
   "source": [
    "La Classe Perceptron rappresenta un singolo neurone ha dei pesi (weights) e un bias (bias), inizializzati con numeri casuali.\n",
    "Inoltre ha un metodo activate(x) che calcola la somma pesata degli input più il bias e applica una funzione di attivazione (come ReLU o Sigmoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373a6ae-48fd-4424-932b-547477b655bc",
   "metadata": {},
   "source": [
    "Con la classe NeuralNetwork creiamo una rete neurale multi-strato usando più Perceptron. Nel costruttore __init__(), definiamo i layer della rete.\n",
    "Ogni layer è una lista di perceptron. Nel metodo forward(), facciamo passare i dati attraverso i layer. Layers è una lista che specifica il numero di neuroni in ogni strato.\n",
    "\n",
    "Esempio: [3, 4, 2] significa: input Layer con 3 neuroni, hidden Layer con 4 neuroni, output Layer con 2 neuroni.\n",
    "\n",
    "Per ogni coppia (input, output), creiamo un array di Perceptron, ognuno con input pesi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1fa45-672b-4fdb-9a1a-2a592c7ff180",
   "metadata": {},
   "source": [
    "Il metodo  forward() calcola il passaggio in avanti nella rete. \n",
    "Per ogni layer: ogni Perceptron calcola il suo output (activate(X)), i risultati vengono passati al layer successivo, la moltiplicazione viene fatta in modo efficiente con np.dot()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c5b21-f996-4c5b-97c0-a43de2039bd0",
   "metadata": {},
   "source": [
    "Con il metodo forward_no_matrix() (BONUS) implementiamo il forward pass senza usare operazioni di matrice. Invece di np.dot(), usiamo un ciclo for: calcoliamo manualmente la somma pesata, applichiamo la funzione di attivazione, salviamo il risultato per il layer successivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8138316-b533-45ab-b18d-9b47a6598111",
   "metadata": {},
   "source": [
    "In sintesi: abbiamo creato una rete neurale usando Perceptron per ogni layer; abbiamo due modi per calcolare l’output: con operazioni di matrice e con cicli for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a87b84-c734-4f2a-a7a1-4fcb6925c758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
